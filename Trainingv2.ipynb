{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPXFTlBKh53vkjj820kAKHF"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["Mounting"],"metadata":{"id":"PKpUP-yiYdJk"}},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5b4szdgFYb97","executionInfo":{"status":"ok","timestamp":1724137126310,"user_tz":-420,"elapsed":44885,"user":{"displayName":"Reihan Hanafi","userId":"13723373612098020183"}},"outputId":"6e174b28-d3a0-4a3c-a0f5-ea97b6d7b750"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["# 1. Mounting drive\n","from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","source":["!pip install --upgrade scikit-learn==1.5.1"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ULQYCdeXYjQW","executionInfo":{"status":"ok","timestamp":1723896738026,"user_tz":-420,"elapsed":11060,"user":{"displayName":"Reihan Hanafi","userId":"13723373612098020183"}},"outputId":"c36cd291-de46-44cb-e9c5-bbcb9e0af98d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: scikit-learn==1.5.1 in /usr/local/lib/python3.10/dist-packages (1.5.1)\n","Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.10/dist-packages (from scikit-learn==1.5.1) (1.26.4)\n","Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn==1.5.1) (1.13.1)\n","Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn==1.5.1) (1.4.2)\n","Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn==1.5.1) (3.5.0)\n"]}]},{"cell_type":"code","source":["import sklearn\n","print(sklearn.__version__)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aNaXJCnDYs8x","executionInfo":{"status":"ok","timestamp":1723896748175,"user_tz":-420,"elapsed":3028,"user":{"displayName":"Reihan Hanafi","userId":"13723373612098020183"}},"outputId":"fa2cdf02-d11e-4892-d643-83522030b852"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["1.5.1\n"]}]},{"cell_type":"code","source":["# Pra Preprocessing Text\n","import nltk\n","from nltk.corpus import stopwords\n","from nltk.stem import WordNetLemmatizer\n","\n","nltk.download('stopwords')\n","nltk.download('wordnet')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JnInLdjhYw4A","executionInfo":{"status":"ok","timestamp":1723877032055,"user_tz":-420,"elapsed":2936,"user":{"displayName":"Reihan Hanafi","userId":"13723373612098020183"}},"outputId":"a8c76a10-ebd6-4af6-d1ef-9441444c6788"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":4}]},{"cell_type":"markdown","source":["PREPROCESSING PER CATEGORY"],"metadata":{"id":"TxuTcO45g7_u"}},{"cell_type":"code","source":["# 2.1 Preprocessing text Adult\n","import pandas as pd\n","import os\n","import re\n","\n","def regeks(teks):\n","    if isinstance(teks, str):\n","        teks = teks.lower()\n","        teks = re.sub(r'<[^>]*>', '', teks)\n","        teks = re.sub(r'http?://\\S+ ?', '', teks)\n","        teks = re.sub(r'[^\\w\\s]', '', teks)\n","        teks = re.sub(r'[0-9]+', ' ', teks)\n","        teks = re.sub(r'[^\\x00-\\x7f]', '', teks)\n","        stop_words = stopwords.words('english') + \\\n","                    stopwords.words('french') + \\\n","                    stopwords.words('russian') + \\\n","                    stopwords.words('indonesian')\n","        teks = ' '.join([word for word in teks.split() if word not in stop_words])\n","        lemmatizer = WordNetLemmatizer()\n","        teks = ' '.join([lemmatizer.lemmatize(word) for word in teks.split()])\n","        return teks\n","    return teks\n","\n","df = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/Web Scraping Reihan/Klasifikasi/Versi 2/Scrape/ADULT_TLS1.csv', lineterminator='\\n', encoding='utf-8')\n","df['contents'] = df['contents'].apply(regeks)\n","print(df['contents'])\n","\n","# Save DataFrame to a CSV file\n","output_folder = '/content/drive/MyDrive/Colab Notebooks/Web Scraping Reihan/Klasifikasi/Versi 2/PrePro'  # Specify the directory where you want to save the CSV\n","if not os.path.exists(output_folder):\n","    os.makedirs(output_folder)\n","\n","output_file = os.path.join(output_folder, 'ADULT_PP.csv')\n","df.to_csv(output_file, index=False)\n","\n","print(f'Data has been saved to {output_file}')\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xO9azNxxY4EO","executionInfo":{"status":"ok","timestamp":1723551668405,"user_tz":-420,"elapsed":10127,"user":{"displayName":"Reihan Hanafi","userId":"13723373612098020183"}},"outputId":"ab487373-4806-4fde-d192-eb4a7b6ada5c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["0      originally posted machetelanding word count pa...\n","1      originally posted lovemusicfashionflawless she...\n","2      originally posted wildeoscars word count pairi...\n","3      originally posted thedrxamsynopsis word count ...\n","4      get experienced future trunk giving girl first...\n","                             ...                        \n","826    aline acrobatic stunt special position adorabl...\n","827    th april photo multiple dick sucking httpssexy...\n","828    lady love mentally dating eddie munson paring ...\n","829    lady love mentally dating eddie munson origina...\n","830    lady love mentally dating eddie munson origina...\n","Name: contents, Length: 831, dtype: object\n","Data has been saved to /content/drive/MyDrive/Colab Notebooks/Web Scraping Reihan/Klasifikasi/Versi 2/PrePro/ADULT_PP.csv\n"]}]},{"cell_type":"code","source":["# 2.1 Preprocessing text Gambling\n","import pandas as pd\n","import os\n","import re\n","\n","def regeks(teks):\n","    if isinstance(teks, str):\n","        teks = teks.lower()\n","        teks = re.sub(r'<[^>]*>', '', teks)\n","        teks = re.sub(r'http?://\\S+ ?', '', teks)\n","        teks = re.sub(r'[^\\w\\s]', '', teks)\n","        teks = re.sub(r'[0-9]+', ' ', teks)\n","        teks = re.sub(r'[^\\x00-\\x7f]', '', teks)\n","        stop_words = stopwords.words('english') + \\\n","                    stopwords.words('french') + \\\n","                    stopwords.words('russian') + \\\n","                    stopwords.words('indonesian')\n","        teks = ' '.join([word for word in teks.split() if word not in stop_words])\n","        lemmatizer = WordNetLemmatizer()\n","        teks = ' '.join([lemmatizer.lemmatize(word) for word in teks.split()])\n","        return teks\n","    return teks\n","\n","df = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/Web Scraping Reihan/Klasifikasi/Versi 2/Scrape/Gambling_TL.csv', lineterminator='\\n', encoding='utf-8')\n","df['contents'] = df['contents'].apply(regeks)\n","print(df['contents'])\n","\n","# Save DataFrame to a CSV file\n","output_folder = '/content/drive/MyDrive/Colab Notebooks/Web Scraping Reihan/Klasifikasi/Versi 2/PrePro'  # Specify the directory where you want to save the CSV\n","if not os.path.exists(output_folder):\n","    os.makedirs(output_folder)\n","\n","output_file = os.path.join(output_folder, 'Gambling_PP.csv')\n","df.to_csv(output_file, index=False)\n","\n","print(f'Data has been saved to {output_file}')\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eRe7BaHkY9Ok","executionInfo":{"status":"ok","timestamp":1723551703067,"user_tz":-420,"elapsed":19395,"user":{"displayName":"Reihan Hanafi","userId":"13723373612098020183"}},"outputId":"de9121d2-10a7-4d61-f2ac-53d17bb4365d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["0      improve user experience use cooky clicking lin...\n","1      get baht clash bonus also lucky spin spin win ...\n","2      net business ltd regent house bisazza street s...\n","3      dream casino founded year experience gambling ...\n","4      free casino game type free casino platform enj...\n","                             ...                        \n","958                                       wildzcasinotop\n","959                   percaya best online site indonesia\n","960    world casino directory casino guide gambling f...\n","961                  world poker tour online poker wptfr\n","962                                            affiliate\n","Name: contents, Length: 963, dtype: object\n","Data has been saved to /content/drive/MyDrive/Colab Notebooks/Web Scraping Reihan/Klasifikasi/Versi 2/PrePro/Gambling_PP.csv\n"]}]},{"cell_type":"code","source":["# 2.1 Preprocessing text Phishing\n","import pandas as pd\n","import os\n","import re\n","\n","def regeks(teks):\n","    if isinstance(teks, str):\n","        teks = teks.lower()\n","        teks = re.sub(r'<[^>]*>', '', teks)\n","        teks = re.sub(r'http?://\\S+ ?', '', teks)\n","        teks = re.sub(r'[^\\w\\s]', '', teks)\n","        teks = re.sub(r'[0-9]+', ' ', teks)\n","        teks = re.sub(r'[^\\x00-\\x7f]', '', teks)\n","        stop_words = stopwords.words('english') + \\\n","                    stopwords.words('french') + \\\n","                    stopwords.words('russian') + \\\n","                    stopwords.words('indonesian')\n","        teks = ' '.join([word for word in teks.split() if word not in stop_words])\n","        lemmatizer = WordNetLemmatizer()\n","        teks = ' '.join([lemmatizer.lemmatize(word) for word in teks.split()])\n","        return teks\n","    return teks\n","\n","df = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/Web Scraping Reihan/Klasifikasi/Versi 2/Scrape/PHISHING_TLS1.csv', lineterminator='\\n', encoding='utf-8')\n","df['contents'] = df['contents'].apply(regeks)\n","print(df['contents'])\n","\n","# Save DataFrame to a CSV file\n","output_folder = '/content/drive/MyDrive/Colab Notebooks/Web Scraping Reihan/Klasifikasi/Versi 2/PrePro'  # Specify the directory where you want to save the CSV\n","if not os.path.exists(output_folder):\n","    os.makedirs(output_folder)\n","\n","output_file = os.path.join(output_folder, 'Phishing_PP.csv')\n","df.to_csv(output_file, index=False)\n","\n","print(f'Data has been saved to {output_file}')\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1aSFEmuaiid3","executionInfo":{"status":"ok","timestamp":1723551716487,"user_tz":-420,"elapsed":6067,"user":{"displayName":"Reihan Hanafi","userId":"13723373612098020183"}},"outputId":"e1968f1b-f226-4b00-861a-e618eb451223"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["0      pomfsu file hosting site using pomf operated l...\n","1      domain name aamaz nfr sale price eur page crea...\n","2      financial success start ab financing solution ...\n","3      plot block th street anna nagar west chennai g...\n","4      add value supply chain superior assembly subas...\n","                             ...                        \n","572    buy domain name dancom youre automatically cov...\n","573    old gloucester street london united kingdom wc...\n","574    domain name zsmk sale price usd page created d...\n","575    domain name zurosportfr sale price eur page cr...\n","576    everything need work together one place explor...\n","Name: contents, Length: 577, dtype: object\n","Data has been saved to /content/drive/MyDrive/Colab Notebooks/Web Scraping Reihan/Klasifikasi/Versi 2/PrePro/Phishing_PP.csv\n"]}]},{"cell_type":"code","source":["# 2.1 Preprocessing text Whitelist\n","import pandas as pd\n","import os\n","import re\n","\n","def regeks(teks):\n","    if isinstance(teks, str):\n","        teks = teks.lower()\n","        teks = re.sub(r'<[^>]*>', '', teks)\n","        teks = re.sub(r'http?://\\S+ ?', '', teks)\n","        teks = re.sub(r'[^\\w\\s]', '', teks)\n","        teks = re.sub(r'[0-9]+', ' ', teks)\n","        teks = re.sub(r'[^\\x00-\\x7f]', '', teks)\n","        stop_words = stopwords.words('english') + \\\n","                    stopwords.words('french') + \\\n","                    stopwords.words('russian') + \\\n","                    stopwords.words('indonesian')\n","        teks = ' '.join([word for word in teks.split() if word not in stop_words])\n","        lemmatizer = WordNetLemmatizer()\n","        teks = ' '.join([lemmatizer.lemmatize(word) for word in teks.split()])\n","        return teks\n","    return teks\n","\n","df = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/Web Scraping Reihan/Klasifikasi/Versi 2/Scrape/Whitelist_TL.csv', lineterminator='\\n', encoding='utf-8')\n","df['contents'] = df['contents'].apply(regeks)\n","print(df['contents'])\n","\n","# Save DataFrame to a CSV file\n","output_folder = '/content/drive/MyDrive/Colab Notebooks/Web Scraping Reihan/Klasifikasi/Versi 2/PrePro'  # Specify the directory where you want to save the CSV\n","if not os.path.exists(output_folder):\n","    os.makedirs(output_folder)\n","\n","output_file = os.path.join(output_folder, 'Whitelist_PP.csv')\n","df.to_csv(output_file, index=False)\n","\n","print(f'Data has been saved to {output_file}')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pcu23SjfkKDj","executionInfo":{"status":"ok","timestamp":1723551755508,"user_tz":-420,"elapsed":8409,"user":{"displayName":"Reihan Hanafi","userId":"13723373612098020183"}},"outputId":"426bb75c-3552-4853-9761-e80e9958793e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["0      one day prince come wellknown song disney univ...\n","1                                              copyright\n","2      yonatan klinger lawyer master degree business ...\n","3      many ghost left lingering city remember left d...\n","4                                                   xcom\n","                             ...                        \n","535    wouldnt surprised know average indonesian spen...\n","536    written josef petrk tldr auth nette extension ...\n","537    contact u zaptank consulting provides followin...\n","538    movie tv serial reality show related informati...\n","539    blog tudes pioneering firm seamlessly merges c...\n","Name: contents, Length: 540, dtype: object\n","Data has been saved to /content/drive/MyDrive/Colab Notebooks/Web Scraping Reihan/Klasifikasi/Versi 2/PrePro/Whitelist_PP.csv\n"]}]},{"cell_type":"markdown","source":["**FEATURE EXTRACTION**"],"metadata":{"id":"pO2MFQ6x3X1b"}},{"cell_type":"code","source":["# 4. Feature Extraction\n","import pandas as pd\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","import joblib\n","import os\n","\n","# Path ke file CSV\n","file_path = '/content/drive/MyDrive/Colab Notebooks/Web Scraping Reihan/Klasifikasi/Versi 2/Dataset_Labelsv2.csv'\n","df = pd.read_csv(file_path)\n","\n","# Membuat objek TfidfVectorizer dengan parameter tertentu\n","tfidf = TfidfVectorizer(\n","    sublinear_tf=True,   # Menggunakan sublinear tf scaling\n","    min_df=5,            # Meminimalkan frekuensi dokumen untuk setiap fitur\n","    norm='l2',           # Normalisasi L2 pada vektor\n","    encoding='latin-1',  # Encoding teks\n","    ngram_range=(1, 2),  # Menggunakan unigram dan bigram\n",")\n","\n","# Mengisi nilai kosong pada kolom 'contents' dengan string kosong ' '\n","Ex = df['contents'].fillna(' ')\n","\n","# Menyesuaikan dan mengubah teks menjadi fitur TF-IDF\n","features = tfidf.fit_transform(Ex).toarray()\n","\n","# Menyiapkan label dari kolom 'category'\n","labels = df['category']\n","\n","# Gabungkan fitur dan label ke dalam DataFrame\n","df_features = pd.DataFrame(features)\n","df_features['label'] = labels.values\n","\n","# Path directory untuk menyimpan file\n","save_path = '/content/drive/MyDrive/Colab Notebooks/Web Scraping Reihan/Klasifikasi/Versi 2/TFIDF/'\n","\n","# Pastikan directory sudah ada, jika belum maka dibuat\n","os.makedirs(save_path, exist_ok=True)\n","\n","# Menyimpan fitur dan label ke dalam file CSV\n","csv_path = os.path.join(save_path, 'Dataset_TFIDF.csv')\n","df_features.to_csv(csv_path, index=False)\n","print(f'Fitur dan label disimpan dalam format CSV di: {csv_path}')\n","\n","# Menyimpan objek TfidfVectorizer dan fitur ke dalam file pkl\n","pkl_path = os.path.join(save_path, 'Data_TFIDF.pkl')\n","joblib.dump((tfidf, features, labels), pkl_path)\n","print(f'Objek TfidfVectorizer dan fitur disimpan dalam format PKL di: {pkl_path}')\n","\n","print(f'Bentuk fitur: {features.shape}')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5Q3Na1pTly5I","executionInfo":{"status":"ok","timestamp":1723557665928,"user_tz":-420,"elapsed":180977,"user":{"displayName":"Reihan Hanafi","userId":"13723373612098020183"}},"outputId":"e5bbcaa8-ffa1-4b48-cde5-6893597c6197"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Fitur dan label disimpan dalam format CSV di: /content/drive/MyDrive/Colab Notebooks/Web Scraping Reihan/Klasifikasi/Versi 2/TFIDF/Dataset_TFIDF.csv\n","Objek TfidfVectorizer dan fitur disimpan dalam format PKL di: /content/drive/MyDrive/Colab Notebooks/Web Scraping Reihan/Klasifikasi/Versi 2/TFIDF/Data_TFIDF.pkl\n","Bentuk fitur: (2911, 36534)\n"]}]},{"cell_type":"markdown","source":["**TRAINING DATA**"],"metadata":{"id":"LTmHteya3lHE"}},{"cell_type":"code","source":["import pandas as pd\n","import joblib\n","from sklearn.model_selection import cross_val_score\n","from sklearn.naive_bayes import MultinomialNB, GaussianNB\n","from sklearn.neighbors import KNeighborsClassifier\n","from sklearn.svm import SVC, LinearSVC\n","from sklearn.multiclass import OneVsRestClassifier\n","\n","# Load dataset from .pkl file\n","pkl_path = '/content/drive/MyDrive/Colab Notebooks/Web Scraping Reihan/Klasifikasi/Versi 2/TFIDF/Data_TFIDF.pkl'\n","tfidf, features, labels = joblib.load(pkl_path)\n","\n","# Define models\n","models = [\n","    MultinomialNB(),\n","    GaussianNB(),\n","    KNeighborsClassifier(n_neighbors=25),\n","    SVC(kernel='linear', C=1),\n","    OneVsRestClassifier(LinearSVC(class_weight=\"balanced\"))\n","]\n","\n","# Cross-validation setup\n","CV = 5\n","cv_df = pd.DataFrame(index=range(CV * len(models)))\n","entries = []\n","\n","for model in models:\n","    model_name = model.__class__.__name__\n","    accuracies = cross_val_score(model, features, labels, scoring='accuracy', cv=CV)\n","    for fold_idx, accuracy in enumerate(accuracies):\n","        entries.append((model_name, fold_idx, accuracy))\n","\n","cv_df = pd.DataFrame(entries, columns=['model_name', 'fold_idx', 'accuracy'])\n","\n","\n","# Find the model with the best average accuracy\n","mean_accuracies = cv_df.groupby('model_name').accuracy.mean()\n","best_model_name = mean_accuracies.idxmax()\n","best_model_accuracy = mean_accuracies.max()\n","\n","print(f\"Best Model: {best_model_name} with Accuracy: {best_model_accuracy:.2%}\")\n","\n","# Train the best model on the entire dataset\n","for model in models:\n","    if model.__class__.__name__ == best_model_name:\n","        best_model = model\n","        best_model.fit(features, labels)\n","        break"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ghG12K1eExaH","executionInfo":{"status":"ok","timestamp":1723560853718,"user_tz":-420,"elapsed":1696317,"user":{"displayName":"Reihan Hanafi","userId":"13723373612098020183"}},"outputId":"21d13211-008b-490b-c849-e8e1708c862f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["              model_name  fold_idx  accuracy\n","0          MultinomialNB         0  0.720412\n","1          MultinomialNB         1  0.725086\n","2          MultinomialNB         2  0.671821\n","3          MultinomialNB         3  0.714777\n","4          MultinomialNB         4  0.697595\n","5             GaussianNB         0  0.691252\n","6             GaussianNB         1  0.697595\n","7             GaussianNB         2  0.658076\n","8             GaussianNB         3  0.733677\n","9             GaussianNB         4  0.639175\n","10  KNeighborsClassifier         0  0.296741\n","11  KNeighborsClassifier         1  0.310997\n","12  KNeighborsClassifier         2  0.298969\n","13  KNeighborsClassifier         3  0.307560\n","14  KNeighborsClassifier         4  0.286942\n","15                   SVC         0  0.838765\n","16                   SVC         1  0.836770\n","17                   SVC         2  0.823024\n","18                   SVC         3  0.845361\n","19                   SVC         4  0.768041\n","20   OneVsRestClassifier         0  0.835334\n","21   OneVsRestClassifier         1  0.836770\n","22   OneVsRestClassifier         2  0.797251\n","23   OneVsRestClassifier         3  0.819588\n","24   OneVsRestClassifier         4  0.764605\n"]}]},{"cell_type":"markdown","source":["**TRAINING DATA + MODEL TERBAIK DI BUAT FILE PICKLE. DATASET TFIDF VERSI PICKLE**"],"metadata":{"id":"g3oRg-gV3rQE"}},{"cell_type":"code","source":["import pandas as pd\n","import joblib\n","from sklearn.model_selection import cross_val_score\n","from sklearn.naive_bayes import MultinomialNB, GaussianNB\n","from sklearn.neighbors import KNeighborsClassifier\n","from sklearn.svm import SVC, LinearSVC\n","from sklearn.multiclass import OneVsRestClassifier\n","\n","# Load dataset from .pkl file\n","pkl_path = '/content/drive/MyDrive/Colab Notebooks/Web Scraping Reihan/Klasifikasi/Versi 2/TFIDF/Data_TFIDF.pkl'\n","tfidf, features, labels = joblib.load(pkl_path)\n","\n","# Define models\n","models = [\n","    MultinomialNB(),\n","    GaussianNB(),\n","    KNeighborsClassifier(n_neighbors=25),\n","    SVC(kernel='linear', C=1),\n","    OneVsRestClassifier(LinearSVC(class_weight=\"balanced\"))\n","]\n","\n","# Cross-validation setup\n","CV = 5\n","cv_df = pd.DataFrame(index=range(CV * len(models)))\n","entries = []\n","\n","# Evaluate each model using cross-validation\n","for model in models:\n","    model_name = model.__class__.__name__\n","    accuracies = cross_val_score(model, features, labels, scoring='accuracy', cv=CV)\n","    for fold_idx, accuracy in enumerate(accuracies):\n","        entries.append((model_name, fold_idx, accuracy))\n","\n","# Store the results in a DataFrame\n","cv_df = pd.DataFrame(entries, columns=['model_name', 'fold_idx', 'accuracy'])\n","\n","# Calculate mean accuracies for each model\n","mean_accuracies = cv_df.groupby('model_name').accuracy.mean()\n","\n","# Print mean accuracies for each model\n","print(\"Average Accuracy for each model:\")\n","for model_name, accuracy in mean_accuracies.items():\n","    print(f\"{model_name}: {accuracy:.2%}\")\n","\n","# Identify the best model\n","best_model_name = mean_accuracies.idxmax()\n","best_model_accuracy = mean_accuracies.max()\n","\n","print(f\"\\nBest Model: {best_model_name} with Accuracy: {best_model_accuracy:.2%}\")\n","\n","# Train the best model on the entire dataset\n","for model in models:\n","    if model.__class__.__name__ == best_model_name:\n","        best_model = model\n","        best_model.fit(features, labels)\n","        break\n","\n","# Hilangkan komentar dibawah ini jika ingin menyimpan file model .pkl\n","# # Simpan model terbaik sebagai file .pkl\n","# pkl_save_path = f'/content/drive/MyDrive/Colab Notebooks/Web Scraping Reihan/Klasifikasi/Versi 2/TFIDF/{best_model_name}.pkl'\n","# joblib.dump(best_model, pkl_save_path)\n","\n","# print(f\"\\nModel Telah disimpan di {pkl_save_path}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vmwKlOTQQwOS","executionInfo":{"status":"ok","timestamp":1723878354995,"user_tz":-420,"elapsed":1312261,"user":{"displayName":"Reihan Hanafi","userId":"13723373612098020183"}},"outputId":"68214db7-addf-44c2-ee26-dc31b6546848"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Average Accuracy for each model:\n","GaussianNB: 68.40%\n","KNeighborsClassifier: 30.02%\n","MultinomialNB: 70.59%\n","OneVsRestClassifier: 81.07%\n","SVC: 82.24%\n","\n","Best Model: SVC with Accuracy: 82.24%\n"]}]},{"cell_type":"code","source":["import pandas as pd\n","import joblib\n","from sklearn.model_selection import cross_val_score\n","from sklearn.naive_bayes import MultinomialNB, GaussianNB\n","from sklearn.neighbors import KNeighborsClassifier\n","from sklearn.svm import SVC, LinearSVC\n","from sklearn.multiclass import OneVsRestClassifier\n","from sklearn.metrics import classification_report\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import accuracy_score\n","\n","# Load dataset from .pkl file\n","pkl_path = '/content/drive/MyDrive/Colab Notebooks/Web Scraping Reihan/Klasifikasi/Versi 2/TFIDF/Data_TFIDF.pkl'\n","tfidf, features, labels = joblib.load(pkl_path)\n","\n","# Define models\n","models = [\n","    MultinomialNB(),\n","    GaussianNB(),\n","    KNeighborsClassifier(n_neighbors=25),\n","    SVC(kernel='linear', C=1),\n","    OneVsRestClassifier(LinearSVC(class_weight=\"balanced\"))\n","]\n","\n","# Cross-validation setup\n","CV = 5\n","cv_df = pd.DataFrame(index=range(CV * len(models)))\n","entries = []\n","\n","# Initialize a dictionary to store predictions and true labels\n","all_predictions = {}\n","all_labels = {}\n","# Evaluate each model using cross-validation\n","for model in models:\n","    model_name = model.__class__.__name__\n","    all_predictions[model_name] = []\n","    all_labels[model_name] = []\n","    for fold_idx in range(CV):\n","        # Split data into training and testing sets for this fold\n","        X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=1/CV, random_state=fold_idx)\n","\n","        # Train the model on the training data\n","        model.fit(X_train, y_train)\n","        # Make predictions on the test data\n","        y_pred = model.predict(X_test)\n","\n","        # Store predictions and true labels for this fold\n","        all_predictions[model_name].extend(y_pred)\n","        all_labels[model_name].extend(y_test)\n","\n","        # Calculate and store accuracy for this fold (you can keep this part if you still need it)\n","        accuracy = accuracy_score(y_test, y_pred)\n","        entries.append((model_name, fold_idx, accuracy))\n","\n","# Print mean accuracies for each model\n","print(\"Average Accuracy for each model:\")\n","for model_name, accuracy in mean_accuracies.items():\n","    print(f\"{model_name}: {accuracy:.2%}\")\n","\n","# Identify the best model\n","best_model_name = mean_accuracies.idxmax()\n","best_model_accuracy = mean_accuracies.max()\n","\n","print(f\"\\nBest Model: {best_model_name} with Accuracy: {best_model_accuracy:.2%}\")\n","\n","# Train the best model on the entire dataset\n","for model in models:\n","    if model.__class__.__name__ == best_model_name:\n","        best_model = model\n","        best_model.fit(features, labels)\n","        break\n","\n","# Make predictions on the entire dataset using the best model\n","predictions = best_model.predict(features)\n","\n","# Generate and print classification reports for all models\n","for model_name in all_predictions:\n","    print(f\"\\nClassification Report for {model_name}:\\n\")\n","    print(classification_report(all_labels[model_name], all_predictions[model_name]))\n","\n","# # Save the best model as a .pkl file\n","# pkl_save_path = f'/content/drive/MyDrive/Colab Notebooks/Web Scraping Reihan/Klasifikasi/Versi 2/TFIDF/{best_model_name}.pkl'\n","# joblib.dump(best_model, pkl_save_path)\n","\n","# print(f\"\\nModel saved to {pkl_save_path}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3OquV-cSEXsq","executionInfo":{"status":"ok","timestamp":1723881909523,"user_tz":-420,"elapsed":1411418,"user":{"displayName":"Reihan Hanafi","userId":"13723373612098020183"}},"outputId":"a5c8ebc0-1bf6-4a85-88f4-204c1d6cf625"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Average Accuracy for each model:\n","GaussianNB: 68.40%\n","KNeighborsClassifier: 30.02%\n","MultinomialNB: 70.59%\n","OneVsRestClassifier: 81.07%\n","SVC: 82.24%\n","\n","Best Model: SVC with Accuracy: 82.24%\n","\n","Classification Report for MultinomialNB:\n","\n","              precision    recall  f1-score   support\n","\n","    Gambling       0.65      0.99      0.79       978\n","    Phishing       0.86      0.49      0.63       535\n","        Porn       0.90      0.89      0.90       857\n","   Whitelist       0.84      0.44      0.58       545\n","\n","    accuracy                           0.77      2915\n","   macro avg       0.82      0.70      0.72      2915\n","weighted avg       0.80      0.77      0.75      2915\n","\n","\n","Classification Report for GaussianNB:\n","\n","              precision    recall  f1-score   support\n","\n","    Gambling       0.77      0.90      0.83       978\n","    Phishing       0.45      0.48      0.46       535\n","        Porn       0.93      0.87      0.90       857\n","   Whitelist       0.64      0.50      0.56       545\n","\n","    accuracy                           0.74      2915\n","   macro avg       0.70      0.68      0.69      2915\n","weighted avg       0.74      0.74      0.73      2915\n","\n","\n","Classification Report for KNeighborsClassifier:\n","\n","              precision    recall  f1-score   support\n","\n","    Gambling       1.00      0.01      0.02       978\n","    Phishing       0.00      0.00      0.00       535\n","        Porn       0.32      0.49      0.38       857\n","   Whitelist       0.20      0.58      0.30       545\n","\n","    accuracy                           0.26      2915\n","   macro avg       0.38      0.27      0.18      2915\n","weighted avg       0.47      0.26      0.18      2915\n","\n","\n","Classification Report for SVC:\n","\n","              precision    recall  f1-score   support\n","\n","    Gambling       0.97      0.90      0.93       978\n","    Phishing       0.77      0.76      0.77       535\n","        Porn       0.86      0.96      0.91       857\n","   Whitelist       0.74      0.70      0.72       545\n","\n","    accuracy                           0.86      2915\n","   macro avg       0.83      0.83      0.83      2915\n","weighted avg       0.86      0.86      0.86      2915\n","\n","\n","Classification Report for OneVsRestClassifier:\n","\n","              precision    recall  f1-score   support\n","\n","    Gambling       0.96      0.91      0.93       978\n","    Phishing       0.77      0.73      0.75       535\n","        Porn       0.87      0.95      0.91       857\n","   Whitelist       0.70      0.70      0.70       545\n","\n","    accuracy                           0.85      2915\n","   macro avg       0.83      0.82      0.82      2915\n","weighted avg       0.85      0.85      0.85      2915\n","\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n","/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n","/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"]}]},{"cell_type":"code","source":["import pandas as pd\n","import joblib\n","from sklearn.model_selection import cross_val_score, train_test_split\n","from sklearn.naive_bayes import MultinomialNB, GaussianNB\n","from sklearn.neighbors import KNeighborsClassifier\n","from sklearn.svm import SVC, LinearSVC\n","from sklearn.multiclass import OneVsRestClassifier\n","from sklearn.metrics import classification_report, accuracy_score\n","\n","# Load dataset from .pkl file\n","pkl_path = '/content/drive/MyDrive/Colab Notebooks/Web Scraping Reihan/Klasifikasi/Versi 2/TFIDF/Data_TFIDF.pkl'\n","tfidf, features, labels = joblib.load(pkl_path)\n","\n","# Define models\n","models = [\n","    MultinomialNB(),\n","    GaussianNB(),\n","    KNeighborsClassifier(n_neighbors=25),\n","    SVC(kernel='linear', C=1),\n","    OneVsRestClassifier(LinearSVC(class_weight=\"balanced\"))\n","]\n","\n","# Cross-validation setup\n","CV = 5\n","cv_df = pd.DataFrame(index=range(CV * len(models)))\n","entries = []\n","\n","# Initialize a dictionary to store predictions and true labels\n","all_predictions = {}\n","all_labels = {}\n","\n","# Evaluate each model using cross-validation and save them\n","for model in models:\n","    model_name = model.__class__.__name__\n","    all_predictions[model_name] = []\n","    all_labels[model_name] = []\n","\n","    # Train the model on the entire dataset (outside the cross-validation loop)\n","    model.fit(features, labels)\n","\n","    # Save the trained model\n","    pkl_save_path = f'/content/drive/MyDrive/Colab Notebooks/Web Scraping Reihan/Klasifikasi/Versi 2/TFIDF/{model_name}.pkl'\n","    joblib.dump(model, pkl_save_path)\n","    print(f\"\\nModel {model_name} saved to {pkl_save_path}\")\n","\n","    accuracies = cross_val_score(model, features, labels, scoring='accuracy', cv=CV)\n","    for fold_idx, accuracy in enumerate(accuracies):\n","        entries.append((model_name, fold_idx, accuracy))\n","\n","    # Split data into training and testing sets for this fold (INSIDE the cross-validation loop)\n","    # X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=1/CV, random_state=fold_idx)\n","\n","    # # Make predictions on the test data (using the already trained model)\n","    # y_pred = model.predict(X_test)\n","\n","    # # Store predictions and true labels for this fold\n","    # all_predictions[model_name].extend(y_pred)\n","    # all_labels[model_name].extend(y_test)\n","\n","    # # Calculate and store accuracy for this fold\n","    # accuracy = accuracy_score(y_test, y_pred)\n","    # entries.append((model_name, fold_idx, accuracy))\n","\n","# Store the results in a DataFrame\n","cv_df = pd.DataFrame(entries, columns=['model_name', 'fold_idx', 'accuracy'])\n","\n","# Calculate mean accuracies for each model\n","mean_accuracies = cv_df.groupby('model_name').accuracy.mean()\n","\n","# Print mean accuracies for each model\n","print(\"Average Accuracy for each model:\")\n","for model_name, accuracy in mean_accuracies.items():\n","    print(f\"{model_name}: {accuracy:.2%}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":707},"id":"SVRu_Y-TO_qV","executionInfo":{"status":"error","timestamp":1723898122757,"user_tz":-420,"elapsed":1362454,"user":{"displayName":"Reihan Hanafi","userId":"13723373612098020183"}},"outputId":"0be7ee2e-2ed7-4a55-9251-5649994e445a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Model MultinomialNB saved to /content/drive/MyDrive/Colab Notebooks/Web Scraping Reihan/Klasifikasi/Versi 2/TFIDF/MultinomialNB.pkl\n","\n","Model GaussianNB saved to /content/drive/MyDrive/Colab Notebooks/Web Scraping Reihan/Klasifikasi/Versi 2/TFIDF/GaussianNB.pkl\n","\n","Model KNeighborsClassifier saved to /content/drive/MyDrive/Colab Notebooks/Web Scraping Reihan/Klasifikasi/Versi 2/TFIDF/KNeighborsClassifier.pkl\n","\n","Model SVC saved to /content/drive/MyDrive/Colab Notebooks/Web Scraping Reihan/Klasifikasi/Versi 2/TFIDF/SVC.pkl\n","\n","Model OneVsRestClassifier saved to /content/drive/MyDrive/Colab Notebooks/Web Scraping Reihan/Klasifikasi/Versi 2/TFIDF/OneVsRestClassifier.pkl\n","Average Accuracy for each model:\n","GaussianNB: 68.40%\n","KNeighborsClassifier: 30.02%\n","MultinomialNB: 70.59%\n","OneVsRestClassifier: 81.07%\n","SVC: 82.24%\n","\n","Classification Report for MultinomialNB:\n","\n"]},{"output_type":"error","ename":"ValueError","evalue":"max() arg is an empty sequence","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-4-aaa9b42048af>\u001b[0m in \u001b[0;36m<cell line: 76>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mmodel_name\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mall_predictions\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"\\nClassification Report for {model_name}:\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclassification_report\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_labels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_predictions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/_param_validation.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    211\u001b[0m                     )\n\u001b[1;32m    212\u001b[0m                 ):\n\u001b[0;32m--> 213\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    214\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mInvalidParameterError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m                 \u001b[0;31m# When the function is just a wrapper around an estimator, we allow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36mclassification_report\u001b[0;34m(y_true, y_pred, labels, target_names, sample_weight, digits, output_dict, zero_division)\u001b[0m\n\u001b[1;32m   2677\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2678\u001b[0m         \u001b[0mlongest_last_line_heading\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"weighted avg\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2679\u001b[0;31m         \u001b[0mname_width\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcn\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtarget_names\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2680\u001b[0m         \u001b[0mwidth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_width\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlongest_last_line_heading\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdigits\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2681\u001b[0m         \u001b[0mhead_fmt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"{:>{width}s} \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\" {:>9}\"\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: max() arg is an empty sequence"]}]},{"cell_type":"markdown","source":["**TRAINING + CLASSIFICATION REPORT. DATASET TFIDF VERSI CSV**"],"metadata":{"id":"bcXEquMg4Co8"}},{"cell_type":"code","source":["import pandas as pd\n","from sklearn.model_selection import train_test_split, cross_val_score\n","from sklearn.naive_bayes import MultinomialNB\n","from sklearn.neighbors import KNeighborsClassifier\n","from sklearn.svm import SVC\n","from sklearn.metrics import classification_report, accuracy_score\n","from sklearn.preprocessing import LabelEncoder\n","import joblib\n","import os\n","\n","# Load the TF-IDF data from the CSV file\n","file_path = '/content/drive/MyDrive/Colab Notebooks/Web Scraping Reihan/Klasifikasi/Versi 2/TFIDF/Dataset_TFIDF.csv'\n","data = pd.read_csv(file_path)\n","\n","# Drop rows with NaN values in labels or features\n","data = data.dropna(subset=['label'])\n","labels = data['label']\n","features = data.drop(columns=['label'])\n","\n","# Fill NaN values in features (if any) with 0\n","features = features.fillna(0)\n","\n","# Split the data into training and testing sets\n","X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=0.2, random_state=42)\n","\n","# Convert string labels to numerical using LabelEncoder\n","label_encoder = LabelEncoder()\n","y_train = label_encoder.fit_transform(y_train)\n","y_test = label_encoder.transform(y_test)\n","\n","# Initialize models\n","nb_model = MultinomialNB()\n","knn_model = KNeighborsClassifier(n_neighbors=5)\n","svm_model = SVC(kernel='linear')\n","\n","# List of models and their names\n","models = [nb_model, knn_model, svm_model]\n","model_names = ['Naive Bayes', 'K-Nearest Neighbors', 'Support Vector Machine']\n","\n","# Cross-validation with training\n","CV = 5\n","entries = []\n","\n","best_model = None\n","best_accuracy = 0\n","best_model_name = \"\"\n","\n","for model, model_name in zip(models, model_names):\n","    print(f\"Evaluating {model_name} Model\")\n","    accuracies = cross_val_score(model, X_train, y_train, scoring='accuracy', cv=CV)\n","    model.fit(X_train, y_train)\n","    predictions = model.predict(X_test)\n","    accuracy = accuracy_score(y_test, predictions)\n","    print(f\"Accuracy: {accuracy:.2%}\") # Format as percentage\n","    print(classification_report(y_test, predictions, target_names=label_encoder.classes_, digits=2))\n","\n","    # The following loop was indented incorrectly, causing the error.\n","    for fold_idx, acc in enumerate(accuracies):\n","        entries.append((model_name, fold_idx, acc))\n","\n","    if accuracy > best_accuracy:\n","        best_accuracy = accuracy\n","        best_model = model\n","        best_model_name = model_name\n","\n","cv_df = pd.DataFrame(entries, columns=['model_name', 'fold_idx', 'accuracy'])\n","\n","print(\"\\nCross-Validation Results:\")\n","print(cv_df.groupby('model_name')['accuracy'].mean().apply(lambda x: \"{:.2%}\".format(x)))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"T37VZEcPh3-r","executionInfo":{"status":"ok","timestamp":1723602013865,"user_tz":-420,"elapsed":1048427,"user":{"displayName":"Reihan Hanafi","userId":"13723373612098020183"}},"outputId":"06f90ca7-4118-4df2-85a2-439216d0b316"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Evaluating Naive Bayes Model\n","Accuracy: 76.84%\n","              precision    recall  f1-score   support\n","\n","    Gambling       0.66      0.99      0.79       200\n","    Phishing       0.90      0.46      0.61       112\n","        Porn       0.93      0.90      0.91       169\n","   Whitelist       0.74      0.45      0.56       102\n","\n","    accuracy                           0.77       583\n","   macro avg       0.81      0.70      0.72       583\n","weighted avg       0.80      0.77      0.75       583\n","\n","Evaluating K-Nearest Neighbors Model\n","Accuracy: 26.24%\n","              precision    recall  f1-score   support\n","\n","    Gambling       1.00      0.07      0.13       200\n","    Phishing       0.88      0.06      0.12       112\n","        Porn       0.97      0.18      0.30       169\n","   Whitelist       0.19      1.00      0.32       102\n","\n","    accuracy                           0.26       583\n","   macro avg       0.76      0.33      0.22       583\n","weighted avg       0.83      0.26      0.21       583\n","\n","Evaluating Support Vector Machine Model\n","Accuracy: 86.96%\n","              precision    recall  f1-score   support\n","\n","    Gambling       0.96      0.92      0.94       200\n","    Phishing       0.80      0.74      0.77       112\n","        Porn       0.88      0.96      0.92       169\n","   Whitelist       0.75      0.75      0.75       102\n","\n","    accuracy                           0.87       583\n","   macro avg       0.85      0.85      0.85       583\n","weighted avg       0.87      0.87      0.87       583\n","\n","\n","Cross-Validation Results:\n","model_name\n","K-Nearest Neighbors       26.07%\n","Naive Bayes               72.51%\n","Support Vector Machine    84.71%\n","Name: accuracy, dtype: object\n"]}]},{"cell_type":"markdown","source":["**BACA MODEL PICKLE**"],"metadata":{"id":"42E6IaRY4Uzk"}},{"cell_type":"code","source":["import joblib\n","\n","# Path to the SVC model file\n","svc_pkl_path = '/content/drive/MyDrive/Colab Notebooks/Web Scraping Reihan/Klasifikasi/Versi 2/TFIDF/SVC.pkl'\n","\n","# Load the SVC model\n","svc_model = joblib.load(svc_pkl_path)\n","\n","# Inspect the model\n","print(\"SVC Model details:\")\n","print(svc_model)\n","\n","# Access and print some relevant attributes\n","print(\"\\nModel's support vectors:\")\n","print(svc_model.support_vectors_[:5])  # Print first 5 support vectors\n","\n","print(\"\\nModel's coefficients:\")\n","print(svc_model.coef_[:5])  # Print first 5 coefficients (for linear kernel)\n","\n","print(\"\\nModel's intercept:\")\n","print(svc_model.intercept_)  # Print the intercept\n","\n","print(\"\\nModel's classes:\")\n","print(svc_model.classes_)  # Print the classes\n","\n","# If needed, check other attributes of the SVC model\n","# For example, if it's a linear SVC, you can access the coefficients and support vectors\n","if hasattr(svc_model, 'coef_'):\n","    print(\"\\nModel's feature names (if available):\")\n","    try:\n","        feature_names = vectorizer.get_feature_names_out()\n","        print(feature_names[:10])  # Print first 10 feature names\n","    except:\n","        print(\"Feature names are not available.\")\n","else:\n","    print(\"Model does not have coef_ attribute, possibly a non-linear SVC.\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lTOEKjWEtAKk","executionInfo":{"status":"ok","timestamp":1723568762142,"user_tz":-420,"elapsed":6768,"user":{"displayName":"Reihan Hanafi","userId":"13723373612098020183"}},"outputId":"3ec6411c-8e9f-4a75-d66c-34071c167015"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["SVC Model details:\n","SVC(C=1, kernel='linear')\n","\n","Model's support vectors:\n","[[0. 0. 0. ... 0. 0. 0.]\n"," [0. 0. 0. ... 0. 0. 0.]\n"," [0. 0. 0. ... 0. 0. 0.]\n"," [0. 0. 0. ... 0. 0. 0.]\n"," [0. 0. 0. ... 0. 0. 0.]]\n","\n","Model's coefficients:\n","[[ 0.11491493  0.06290193  0.02221418 ... -0.02634481  0.\n","   0.        ]\n"," [ 0.05189887  0.10458166  0.         ...  0.09404513  0.\n","   0.        ]\n"," [ 0.02399365  0.0979533   0.0383275  ...  0.12689767 -0.00187289\n","  -0.00194232]\n"," [ 0.          0.          0.         ...  0.06324726  0.\n","   0.        ]\n"," [-0.06704176  0.05314664  0.         ...  0.235968   -0.00990096\n","  -0.00709272]]\n","\n","Model's intercept:\n","[-0.32404764 -0.4854843  -0.46750652 -0.28010752 -0.11610672  0.15003996]\n","\n","Model's classes:\n","['Gambling' 'Phishing' 'Porn' 'Whitelist']\n","\n","Model's feature names (if available):\n","['aa' 'aaa' 'aams' 'aaron' 'ab' 'abandon' 'abandoned' 'abandoned building'\n"," 'abandoned soon' 'abandoning']\n"]}]},{"cell_type":"code","source":["import joblib  # Use joblib for loading models, especially scikit-learn models\n","\n","# Path to the .pkl file in your Google Drive\n","data = '/content/drive/MyDrive/Colab Notebooks/Web Scraping Reihan/Klasifikasi/Versi 2/TFIDF/SVC.pkl'\n","\n","# Load model using joblib\n","model = joblib.load(data)"],"metadata":{"id":"ZmfOpgjLjC0B"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(model.classes_)  # Menampilkan kelas-kelas yang ada\n","print(model.support_vectors_)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2NPUljoSkFU5","executionInfo":{"status":"ok","timestamp":1723801297042,"user_tz":-420,"elapsed":398,"user":{"displayName":"Reihan Hanafi","userId":"13723373612098020183"}},"outputId":"95edf7d5-7d24-4259-acd4-95192d943594"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["['Gambling' 'Phishing' 'Porn' 'Whitelist']\n","[[0. 0. 0. ... 0. 0. 0.]\n"," [0. 0. 0. ... 0. 0. 0.]\n"," [0. 0. 0. ... 0. 0. 0.]\n"," ...\n"," [0. 0. 0. ... 0. 0. 0.]\n"," [0. 0. 0. ... 0. 0. 0.]\n"," [0. 0. 0. ... 0. 0. 0.]]\n"]}]},{"cell_type":"markdown","source":["TEST MODEL"],"metadata":{"id":"xCEKGcf32kmx"}},{"cell_type":"code","source":["# prompt: prediksi data tfidf dan model. inputnya adalah URL, berarti dia harus scrape berikan full code lengkap\n","\n","from google.colab import drive\n","import sklearn\n","import nltk\n","from nltk.corpus import stopwords\n","from nltk.stem import WordNetLemmatizer\n","import pandas as pd\n","import os\n","import re\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","import joblib\n","from sklearn.model_selection import cross_val_score\n","from sklearn.naive_bayes import MultinomialNB, GaussianNB\n","from sklearn.neighbors import KNeighborsClassifier\n","from sklearn.svm import SVC, LinearSVC\n","from sklearn.multiclass import OneVsRestClassifier\n","from sklearn.model_selection import train_test_split, cross_val_score\n","from sklearn.naive_bayes import MultinomialNB\n","from sklearn.svm import SVC\n","from sklearn.metrics import classification_report, accuracy_score\n","from sklearn.preprocessing import LabelEncoder\n","import numpy as np\n","import joblib  # Use joblib for loading models, especially scikit-learn models\n","import requests\n","from bs4 import BeautifulSoup\n","import time\n","from IPython.display import display, HTML\n","from IPython.display import HTML\n","\n","# 1. Memuat data TF-IDF dan model yang sudah dilatih\n","# Path ke data TF-IDF dan model terbaik Anda\n","tfidf_data_path = '/content/drive/MyDrive/Colab Notebooks/Web Scraping Reihan/Klasifikasi/Versi 2/TFIDF/Data_TFIDF.pkl'\n","model_path = '/content/drive/MyDrive/Colab Notebooks/Web Scraping Reihan/Klasifikasi/Versi 2/TFIDF/SVC.pkl'\n","\n","# Muat data TF-IDF dan model menggunakan joblib\n","tfidf, features, labels = joblib.load(tfidf_data_path)\n","model = joblib.load(model_path)\n","\n","# 2. Fungsi untuk pra-pemrosesan teks baru\n","def regeks(teks):\n","    if isinstance(teks, str):\n","        teks = teks.lower()  # Ubah teks menjadi huruf kecil\n","        teks = re.sub(r'<[^>]*>', '', teks)  # Hapus tag HTML\n","        teks = re.sub(r'http?://\\S+ ?', '', teks)  # Hapus URL\n","        teks = re.sub(r'[^\\w\\s]', '', teks)  # Hapus tanda baca\n","        teks = re.sub(r'[0-9]+', ' ', teks)  # Hapus angka\n","        teks = re.sub(r'[^\\x00-\\x7f]', '', teks)  # Hapus karakter non-ASCII\n","\n","        # Gabungkan daftar stop words dari beberapa bahasa\n","        stop_words = stopwords.words('english') + \\\n","                     stopwords.words('french') + \\\n","                     stopwords.words('russian') + \\\n","                     stopwords.words('indonesian')\n","\n","        # Hapus stop words dan lakukan lemmatization\n","        teks = ' '.join([word for word in teks.split() if word not in stop_words])\n","        lemmatizer = WordNetLemmatizer()\n","        teks = ' '.join([lemmatizer.lemmatize(word) for word in teks.split()])\n","        return teks\n","    return teks\n","\n","# 3. Fungsi untuk mengambil konten dari URL\n","def scrape_url(url):\n","    if not url.startswith(\"http://\") and not url.startswith(\"https://\"):\n","        url = \"https://\" + url  # Tambahkan https:// jika protokol tidak ditentukan\n","\n","    headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/126.0.0.0 Safari/537.36'}\n","    for attempt in range(2):  # Coba hingga 2 kali\n","        try:\n","            response = requests.get(url, headers=headers, timeout=10)\n","            response.raise_for_status()  # Pastikan permintaan berhasil\n","            return response.content\n","        except requests.exceptions.RequestException as e:\n","            print(f\"Percobaan {attempt + 1} untuk {url} gagal dengan kesalahan: {e}. Mencoba lagi...\")\n","            time.sleep(2)  # Tunggu 2 detik sebelum mencoba lagi\n","    return None\n","\n","# 4. Masukkan URL untuk prediksi\n","url_to_predict = \"bunghatta.ac.id\"  # Ganti dengan URL yang ingin Anda prediksi\n","\n","# 5. Ambil konten dari URL\n","content = scrape_url(url_to_predict)\n","\n","if content is None:\n","    print(f\"Gagal mengambil konten dari {url_to_predict}\")\n","else:\n","    soup = BeautifulSoup(content, \"html.parser\")\n","\n","    # Hapus semua kode JavaScript dan stylesheet\n","    for script in soup([\"script\", \"style\"]):\n","        script.extract()\n","\n","    # Ekstrak konten teks dari halaman web\n","    text_content = soup.get_text(separator=' ')\n","\n","    # 6. Pra-pemrosesan teks yang diambil\n","    processed_text = regeks(text_content)\n","\n","    # 7. Transformasi teks yang sudah diproses menggunakan TF-IDF\n","    new_text_features = tfidf.transform([processed_text]).toarray()\n","\n","    # 8. Lakukan prediksi\n","    prediction = model.predict(new_text_features)\n","\n","    # 9. Dapatkan kategori yang diprediksi (dengan asumsi prediksi langsung berupa label kategori)\n","    predicted_category = prediction[0]\n","\n","    # 10. Cetak hasil\n","    print(f\"URL Terdeteksi untuk {url_to_predict}: {predicted_category}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RuOqBsovqSTp","executionInfo":{"status":"ok","timestamp":1723806932995,"user_tz":-420,"elapsed":5173,"user":{"displayName":"Reihan Hanafi","userId":"13723373612098020183"}},"outputId":"d20f2f7d-bc1f-409b-8151-fe84d8f07d78"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["URL Terdeteksi untuk bunghatta.ac.id: Whitelist\n"]}]},{"cell_type":"code","source":["!pip install flask"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dM5HdOq_PKun","executionInfo":{"status":"ok","timestamp":1723812599103,"user_tz":-420,"elapsed":8074,"user":{"displayName":"Reihan Hanafi","userId":"13723373612098020183"}},"outputId":"5dc3b4bc-7067-476c-b8f2-6a810135a6d1"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: flask in /usr/local/lib/python3.10/dist-packages (2.2.5)\n","Requirement already satisfied: Werkzeug>=2.2.2 in /usr/local/lib/python3.10/dist-packages (from flask) (3.0.3)\n","Requirement already satisfied: Jinja2>=3.0 in /usr/local/lib/python3.10/dist-packages (from flask) (3.1.4)\n","Requirement already satisfied: itsdangerous>=2.0 in /usr/local/lib/python3.10/dist-packages (from flask) (2.2.0)\n","Requirement already satisfied: click>=8.0 in /usr/local/lib/python3.10/dist-packages (from flask) (8.1.7)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from Jinja2>=3.0->flask) (2.1.5)\n"]}]},{"cell_type":"code","source":["# prompt: buatkan flask, inputnya adalah URL. halaman utamanya adalah index.html, hasil prediksinya pada kolom kategori, seperti code prediksi sebelumnya. berikan code full lengkap\n","\n","from flask import Flask, request, render_template\n","import joblib\n","import re\n","import requests\n","from bs4 import BeautifulSoup\n","import time\n","from nltk.corpus import stopwords\n","from nltk.stem import WordNetLemmatizer\n","\n","app = Flask(__name__)\n","\n","# Load the TF-IDF vectorizer and trained model\n","tfidf_data_path = '/content/drive/MyDrive/Colab Notebooks/Web Scraping Reihan/Klasifikasi/Versi 2/TFIDF/Data_TFIDF.pkl'\n","model_path = '/content/drive/MyDrive/Colab Notebooks/Web Scraping Reihan/Klasifikasi/Versi 2/TFIDF/SVC.pkl'\n","tfidf, _, _ = joblib.load(tfidf_data_path)\n","model = joblib.load(model_path)\n","\n","# Preprocessing function\n","def regeks(teks):\n","    if isinstance(teks, str):\n","        teks = teks.lower()\n","        teks = re.sub(r'<[^>]*>', '', teks)\n","        teks = re.sub(r'http?://\\S+ ?', '', teks)\n","        teks = re.sub(r'[^\\w\\s]', '', teks)\n","        teks = re.sub(r'[0-9]+', ' ', teks)\n","        teks = re.sub(r'[^\\x00-\\x7f]', '', teks)\n","        stop_words = stopwords.words('english') + \\\n","                     stopwords.words('french') + \\\n","                     stopwords.words('russian') + \\\n","                     stopwords.words('indonesian')\n","        teks = ' '.join([word for word in teks.split() if word not in stop_words])\n","        lemmatizer = WordNetLemmatizer()\n","        teks = ' '.join([lemmatizer.lemmatize(word) for word in teks.split()])\n","        return teks\n","    return teks\n","\n","# Function to scrape content from URL\n","def scrape_url(url):\n","    if not url.startswith(\"http://\") and not url.startswith(\"https://\"):\n","        url = \"https://\" + url\n","    headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/126.0.0.0 Safari/537.36'}\n","    for attempt in range(2):\n","        try:\n","            response = requests.get(url, headers=headers, timeout=10)\n","            response.raise_for_status()\n","            return response.content\n","        except requests.exceptions.RequestException as e:\n","            print(f\"Attempt {attempt + 1} for {url} failed with error: {e}. Retrying...\")\n","            time.sleep(2)\n","    return None\n","\n","# Route for the index page\n","@app.route('/')\n","def index():\n","    return render_template('index.html')\n","\n","# Route to handle prediction requests\n","@app.route('/predict', methods=['POST'])\n","def predict():\n","    url = request.form['url']\n","    content = scrape_url(url)\n","    if content is None:\n","        return render_template('index.html', error=f\"Failed to retrieve content from {url}\")\n","\n","    soup = BeautifulSoup(content, \"html.parser\")\n","    for script in soup([\"script\", \"style\"]):\n","        script.extract()\n","    text_content = soup.get_text(separator=' ')\n","    processed_text = regeks(text_content)\n","    new_text_features = tfidf_vectorizer.transform([processed_text]).toarray()\n","    prediction = model.predict(new_text_features)\n","    predicted_category = prediction[0]\n","    return render_template('index.html', prediction=predicted_category, url=url)\n","\n","if __name__ == '__main__':\n","    app.run(debug=True)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"g67Dhry2OcZB","executionInfo":{"status":"ok","timestamp":1723815356180,"user_tz":-420,"elapsed":1540283,"user":{"displayName":"Reihan Hanafi","userId":"13723373612098020183"}},"outputId":"dc450505-84d8-402a-9bef-818f46d89dcd"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":[" * Serving Flask app '__main__'\n"," * Debug mode: on\n"]},{"output_type":"stream","name":"stderr","text":["INFO:werkzeug:\u001b[31m\u001b[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\u001b[0m\n"," * Running on http://127.0.0.1:5000\n","INFO:werkzeug:\u001b[33mPress CTRL+C to quit\u001b[0m\n","INFO:werkzeug: * Restarting with stat\n"]}]},{"cell_type":"code","source":["# # prompt: prediksi data tfidf dan model. berikan full code lengkap\n","# import sklearn\n","# import nltk\n","# from nltk.corpus import stopwords\n","# from nltk.stem import WordNetLemmatizer\n","# import pandas as pd\n","# import os\n","# import re\n","# from sklearn.feature_extraction.text import TfidfVectorizer\n","# import joblib\n","# from sklearn.model_selection import cross_val_score\n","# from sklearn.naive_bayes import MultinomialNB, GaussianNB\n","# from sklearn.neighbors import KNeighborsClassifier\n","# from sklearn.svm import SVC, LinearSVC\n","# from sklearn.multiclass import OneVsRestClassifier\n","# from sklearn.model_selection import train_test_split, cross_val_score\n","# from sklearn.naive_bayes import MultinomialNB\n","# from sklearn.svm import SVC\n","# from sklearn.metrics import classification_report, accuracy_score\n","# from sklearn.preprocessing import LabelEncoder\n","# import numpy as np\n","# import joblib  # Use joblib for loading models, especially scikit-learn models\n","\n","\n","# # 2. Load the TF-IDF data and the trained model\n","# tfidf_data_path = '/content/drive/MyDrive/Colab Notebooks/Web Scraping Reihan/Klasifikasi/Versi 2/TFIDF/Data_TFIDF.pkl'\n","# model_path = '/content/drive/MyDrive/Colab Notebooks/Web Scraping Reihan/Klasifikasi/Versi 2/TFIDF/SVC.pkl'  # Path to your best model\n","\n","# tfidf, features, labels = joblib.load(tfidf_data_path)\n","# model = joblib.load(model_path)\n","\n","# # 3. Function for preprocessing new text\n","# def regeks(teks):\n","#     if isinstance(teks, str):\n","#         teks = teks.lower()\n","#         teks = re.sub(r'<[^>]*>', '', teks)\n","#         teks = re.sub(r'http?://\\S+ ?', '', teks)\n","#         teks = re.sub(r'[^\\w\\s]', '', teks)\n","#         teks = re.sub(r'[0-9]+', ' ', teks)\n","#         teks = re.sub(r'[^\\x00-\\x7f]', '', teks)\n","#         stop_words = stopwords.words('english') + \\\n","#                     stopwords.words('french') + \\\n","#                     stopwords.words('russian') + \\\n","#                     stopwords.words('indonesian')\n","#         teks = ' '.join([word for word in teks.split() if word not in stop_words])\n","#         lemmatizer = WordNetLemmatizer()\n","#         teks = ' '.join([lemmatizer.lemmatize(word) for word in teks.split()])\n","#         return teks\n","#     return teks\n","\n","# # 4. Input new text for prediction\n","# new_text = \"Originally posted by machetelanding  Word count: 669 Pairing: Ash Williams x reader Warnings: Age play, adult themes, smut  Come to Daddy! Ash coaxes. You head over to him and sit on his lap. Sure, hes not the most handsome or in shape guy, but for someone in his late 50s, you are enamored with him. He makes you laugh, buys you little gifts, pleases you well andhe accepts your kink. Hi Daddy. What are we going to do today? You ask him as you bat your eyelashes. Its been since you were 18 that you decided you wanted the attention of older men. The guys your age just couldnt compete. They left you feeling unsatisfied with their fibs of knowing how to please a woman. Most of them couldnt even locate the clitoris, let alone give a woman an orgasm. You know what were going to do today, princess. His hands wrap themselves around your waist as he places wet kisses all over your neck. You squeal softly at his attention.\"\n","\n","# # 5. Preprocess the new text\n","# processed_text = regeks(new_text)\n","\n","# # 6. Transform the preprocessed text using TF-IDF\n","# new_text_features = tfidf.transform([processed_text]).toarray()\n","\n","# # 7. Make the prediction\n","# prediction = model.predict(new_text_features)\n","\n","# # 8. Get the predicted category (assuming prediction is directly the category label)\n","# predicted_category = prediction\n","\n","# # 9. Print the result\n","# print(f\"Predicted Category: {predicted_category}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EzT0XOqplYHn","executionInfo":{"status":"ok","timestamp":1723802479506,"user_tz":-420,"elapsed":10072,"user":{"displayName":"Reihan Hanafi","userId":"13723373612098020183"}},"outputId":"47779f40-378e-485c-95fb-644b2efaaa05"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Predicted Category: ['Porn']\n"]}]}]}